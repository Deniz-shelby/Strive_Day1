{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import torch\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# load dataset\n",
    "ag_dataset = load_dataset('ag_news')\n",
    "\n",
    "# Create a train/dev/test splits\n",
    "ag_dev_dataset = load_dataset('ag_news', split='train[10%:11%]')\n",
    "ag_train_dataset = load_dataset('ag_news', split='train[:10%]')\n",
    "ag_test_dataset = load_dataset('ag_news', split='test[11%:12%]')\n",
    "\n",
    "# merge the splits in a single `datasets.DatasetDict` object\n",
    "ag_split = {split: data for split, data in zip(['train', 'test', 'dev'], [ag_train_dataset, ag_test_dataset, ag_dev_dataset])}\n",
    "ag_dataset_split =  datasets.DatasetDict(ag_split)\n",
    "\n",
    "# Count the number of labels.\n",
    "# Important: use all the splits to compute the labels. \n",
    "num_labels = len(set(ag_dataset_split['train'].unique('label') + \n",
    "                     ag_dataset_split['test'].unique('label') +\n",
    "                     ag_dataset_split['dev'].unique('label')))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/deniz/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/deniz/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/deniz/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/deniz/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model_pretrained = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pretrained)\n",
    "\n",
    "\"\"\"ðŸ¤” **Understanding BERT tokenizer**\"\"\"\n",
    "\n",
    "# Define a tokenization function for the dataset used a standard for text classification\n",
    "def tokenize(dataset):\n",
    "    sentences = dataset['text']\n",
    "    return tokenizer(sentences, \n",
    "                     padding='max_length',\n",
    "                     truncation=True)\n",
    "    \n",
    "# apply it \n",
    "ag_dataset_tokenized = ag_dataset_split.map(tokenize,\n",
    "                                            batched=True,\n",
    "                                            remove_columns=['text'],\n",
    "                                            desc='Tokenize data')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/deniz/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-202c20a3ee59b4d0.arrow\n",
      "Tokenize data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.46ba/s]\n",
      "Loading cached processed dataset at /home/deniz/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-0a9fc35bce44a8ca.arrow\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# load model\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "checkpoint = 'bert-ag-news-classification'\n",
    "model = BertForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "test_data = ag_dataset_tokenized['test']\n",
    "display(test_data)\n",
    "print(type(test_data))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'token_type_ids'],\n",
       "    num_rows: 76\n",
       "})"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "\n",
    "X = test_data.pop['label']\n",
    "y = test_data['label']\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'pop'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-64cbc215b898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# x = ['inpput_ids]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# y = ['label'] len 76\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'pop'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "test_trainer = Trainer(model) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "raw_pred = test_trainer.predict(test_data) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 76\n",
      "  Batch size = 8\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:01<00:00,  7.40it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "y_true = test_data['label']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Step 5: Making prediction\n",
    "######## Tokenize test data\n",
    "\n",
    "#X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "#  \n",
    "########## Create torch dataset\n",
    "\n",
    "#test_dataset = Dataset(X_test_tokenized) \n",
    "\n",
    "############ Load trained model\n",
    "\n",
    "#model_path = \"output/checkpoint-50000\"\n",
    "#model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2) \n",
    "\n",
    "############## Define test trainer\n",
    "\n",
    "#test_trainer = Trainer(model) \n",
    "\n",
    "############ Make prediction\n",
    "\n",
    "#raw_pred, _, _ = test_trainer.predict(test_dataset) \n",
    "\n",
    "############ Preprocess raw predictions\n",
    "\n",
    "#y_pred = np.argmax(raw_pred, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('NLP': conda)"
  },
  "interpreter": {
   "hash": "86c899ea07089cc9d284ea9f9b0758fa4e79f644b216f9a03b961d7e004425f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}